{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f494785",
   "metadata": {},
   "source": [
    "Getting packages and reading file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5240990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langdetect\n",
    "!pip install pycountry\n",
    "!pip install textblob\n",
    "!pip install tweepy\n",
    "!pip3 install wordcloud\n",
    "from textblob import TextBlob\n",
    "import sys\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import pycountry\n",
    "import re\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from langdetect import detect\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "#reading csv file containing scrapped tweets:\n",
    "tweet_list = pd.read_csv(r\"name.csv\")\n",
    "\n",
    "#dropping duplicate tweets from Tweets column:\n",
    "\n",
    "tweet_list.drop_duplicates(subset=['Tweets'] ,keep = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10de4702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage(part,whole):\n",
    "     return 100 * float(part)/float(whole)\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#function to cleaning the tweets\n",
    "def clean_tweet(Tweets):\n",
    "    if type(Tweets) == np.float:\n",
    "        return \"\"\n",
    "    temp = Tweets.lower()\n",
    "    temp = re.sub(\"'\", \"\", temp) # to avoid removing contractions in english\n",
    "    temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp)\n",
    "    temp = re.sub(\"#[A-Za-z0-9_]+\",\"\", temp)\n",
    "    temp = re.sub(r'http\\S+', '', temp)\n",
    "    temp = re.sub('[()!?]', ' ', temp)\n",
    "    temp = re.sub('\\[.*?\\]',' ', temp)\n",
    "    temp = re.sub(\"[^a-z0-9]\",\" \", temp)\n",
    "    temp = temp.split()\n",
    "    temp = [w for w in temp if not w in stop_words]\n",
    "    temp = \" \".join(temp)\n",
    "    temp = temp.replace('RT','')\n",
    "    temp = temp.replace('rt','')\n",
    "    return temp\n",
    "\n",
    "#function to keep only first 100 characters of the tweets\n",
    "def shorten_tweets(Tweets):\n",
    "    temp = Tweets[0:100]\n",
    "    return temp\n",
    "ID_TWEET = pd.DataFrame(tweet_list)\n",
    "ID_TWEET['clean_tweets'] = ID_TWEET['Tweets']\n",
    "#Removing RT, Punctuation etc:\n",
    "ID_TWEET['clean_tweets'] = ID_TWEET['clean_tweets'].apply(clean_tweet)\n",
    "ID_TWEET['clean_tweets'] = ID_TWEET['clean_tweets'].apply(clean_tweet)\n",
    "\n",
    "#adding new column to the dataframe and checking for duplicates:\n",
    "ID_TWEET['first100charactersoftweets'] =ID_TWEET['clean_tweets'].apply(shorten_tweets)\n",
    "ID_TWEET = ID_TWEET.drop_duplicates(subset='first100charactersoftweets', keep=\"first\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
